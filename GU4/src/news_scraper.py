# -*- coding: utf-8 -*-
"""
å°ç£è²¡ç¶“æ–°èçˆ¬èŸ²ï¼ˆæ›¿ä»£ Google CSEï¼‰
ç›´æ¥æŠ“å–é‰…äº¨ç¶²ã€ç¶“æ¿Ÿæ—¥å ±ç­‰æ–°èç¶²ç«™
"""
import logging
import requests
from typing import List, Dict
from datetime import datetime
from bs4 import BeautifulSoup
import time

logger = logging.getLogger(__name__)


class TaiwanNewsScaper:
    """å°ç£è²¡ç¶“æ–°èçˆ¬èŸ²"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://news.cnyes.com/',
            'Cache-Control': 'max-age=0'
        }
    
    def search_stock_news(
        self, 
        stock_code: str, 
        stock_name: str,
        max_results: int = 5
    ) -> List[Dict[str, str]]:
        """
        æœç´¢è‚¡ç¥¨ç›¸é—œæ–°è
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç¢¼
            stock_name: è‚¡ç¥¨åç¨±
            max_results: æœ€å¤šè¿”å›å¹¾å‰‡æ–°è
            
        Returns:
            æ–°èåˆ—è¡¨
        """
        news_list = []
        
        # 1. æŠ“å– Yahoo è²¡ç¶“ RSSï¼ˆæœ€ç©©å®šï¼‰
        try:
            yahoo_news = self._fetch_yahoo_rss(stock_code, stock_name)
            news_list.extend(yahoo_news)
        except Exception as e:
            logger.debug(f"Yahoo RSS å¤±æ•—: {e}")
        
        # 2. æŠ“å–ç¶“æ¿Ÿæ—¥å ±æ–°è
        try:
            udn_news = self._fetch_udn_news(stock_name)
            news_list.extend(udn_news)
        except Exception as e:
            logger.debug(f"ç¶“æ¿Ÿæ—¥å ±å¤±æ•—: {e}")
        
        # 3. å¦‚æœæ²’æœ‰æ–°èï¼Œè¿”å›é€šç”¨å°è‚¡æ–°è
        if not news_list:
            news_list = self._fetch_general_tw_stock_news(stock_name)
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        seen = set()
        unique_news = []
        for news in news_list:
            if news['title'] not in seen:
                seen.add(news['title'])
                unique_news.append(news)
                if len(unique_news) >= max_results:
                    break
        
        logger.info(f"[{stock_code}] æ‰¾åˆ° {len(unique_news)} å‰‡æ–°è")
        return unique_news
    
    def _fetch_udn_news(self, stock_name: str) -> List[Dict]:
        """æŠ“å–ç¶“æ¿Ÿæ—¥å ±å°è‚¡æ–°èï¼ˆç©©å®šä¾†æºï¼‰"""
        try:
            url = "https://money.udn.com/rssfeed/news/1001/5591"  # å°è‚¡æ–°è RSS
            
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'xml')
            items = soup.find_all('item', limit=20)
            
            news_list = []
            for item in items:
                try:
                    title = item.find('title').text if item.find('title') else ''
                    link = item.find('link').text if item.find('link') else ''
                    description = item.find('description').text if item.find('description') else ''
                    
                    # ç¯©é¸ç›¸é—œæ–°èï¼ˆå¯¬é¬†æ¢ä»¶ï¼‰
                    if stock_name in title or stock_name in description or self._match_keywords(title):
                        news_list.append({
                            'title': title[:100],
                            'snippet': description[:150] if description else '',
                            'link': link,
                            'source': 'ç¶“æ¿Ÿæ—¥å ±'
                        })
                except Exception:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.debug(f"ç¶“æ¿Ÿæ—¥å ±éŒ¯èª¤: {e}")
            return []
        """æŠ“å–é‰…äº¨ç¶²æ–°èï¼ˆå…¬é–‹ APIï¼‰"""
        try:
            # é‰…äº¨ç¶²å°è‚¡æ–°èåˆ—è¡¨
            url = "https://news.cnyes.com/news/cat/tw_stock"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                logger.debug(f"é‰…äº¨ç¶²ç„¡æ³•è¨ªå•: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_list = []
            
            # å°‹æ‰¾æ–°èé …ç›®
            articles = soup.find_all('a', class_=['_1Zdp'], limit=20)
            if not articles:
                # å‚™ç”¨é¸æ“‡å™¨
                articles = soup.find_all('a', limit=30)
            
            for article in articles:
                try:
                    title = article.text.strip()
                    link = article.get('href', '')
                    
                    # ç¯©é¸åŒ…å«è‚¡ç¥¨ç›¸é—œçš„æ–°è
                    if len(title) > 15 and (stock_name in title or stock_code in title or self._match_keywords(title)):
                        if link and not link.startswith('http'):
                            link = 'https://news.cnyes.com' + link
                        
                        if link.startswith('http'):
                            news_list.append({
                                'title': title[:100],
                                'snippet': '',
                                'link': link,
                                'source': 'é‰…äº¨ç¶²'
                            })
                except Exception:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.debug(f"é‰…äº¨ç¶²éŒ¯èª¤: {e}")
            return []
    
    def _fetch_yahoo_rss(self, stock_code: str, stock_name: str) -> List[Dict]:
        """æŠ“å– Yahoo è²¡ç¶“æ–°è"""
        try:
            # Yahoo å°è‚¡æ–°è RSS
            url = "https://tw.stock.yahoo.com/rss"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'xml')
            items = soup.find_all('item', limit=20)
            
            news_list = []
            for item in items:
                try:
                    title = item.find('title').text if item.find('title') else ''
                    link = item.find('link').text if item.find('link') else ''
                    description = item.find('description').text if item.find('description') else ''
                    
                    # ç¯©é¸ç›¸é—œæ–°è
                    if stock_name in title or stock_code in title or stock_name in description:
                        news_list.append({
                            'title': title,
                            'snippet': description[:150],
                            'link': link,
                            'source': 'Yahooè‚¡å¸‚'
                        })
                except Exception:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.debug(f"Yahoo RSS éŒ¯èª¤: {e}")
            return []
    
    def _fetch_general_tw_stock_news(self, stock_name: str) -> List[Dict]:
        """æŠ“å–å°è‚¡é€šç”¨æ–°èï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            # ç¶“æ¿Ÿæ—¥å ±å°è‚¡æ–°è
            url = "https://money.udn.com/money/cate/5591"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_list = []
            
            # æ‰¾æ–°èæ¨™é¡Œ
            articles = soup.find_all('a', limit=15)
            
            for article in articles:
                try:
                    title = article.text.strip()
                    link = article.get('href', '')
                    
                    # åŸºæœ¬ç¯©é¸
                    if len(title) > 15 and ('å°è‚¡' in title or 'è‚¡å¸‚' in title or stock_name in title):
                        if link and not link.startswith('http'):
                            link = 'https://money.udn.com' + link
                        
                        if link.startswith('http'):
                            news_list.append({
                                'title': title[:100],
                                'snippet': f'ç›¸é—œæ–°èï¼š{stock_name}',
                                'link': link,
                                'source': 'ç¶“æ¿Ÿæ—¥å ±'
                            })
                except Exception:
                    continue
            
            return news_list[:3]  # æœ€å¤š3å‰‡é€šç”¨æ–°è
            
        except Exception as e:
            logger.debug(f"ç¶“æ¿Ÿæ—¥å ±éŒ¯èª¤: {e}")
            return []
    
    def _match_keywords(self, text: str) -> bool:
        """åŒ¹é…å°è‚¡ç›¸é—œé—œéµå­—"""
        keywords = ['å°è‚¡', 'è‚¡å¸‚', 'ä¸Šå¸‚', 'ä¸Šæ«ƒ', 'å¤§ç›¤', 'å°ç£50', 'ç§‘æŠ€è‚¡', 'é›»å­è‚¡', 'åŠå°é«”']
        return any(keyword in text for keyword in keywords)
        """æŠ“å–é‰…äº¨ç¶²æ–°è (ä½¿ç”¨ API)"""
        try:
            # é‰…äº¨ç¶²æ–°è APIï¼ˆå…¬é–‹æ¥å£ï¼‰
            url = f"https://api.cnyes.com/media/api/v1/newslist/category/tw_stock"
            params = {
                'limit': 10,
                'page': 1
            }
            
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return []
            
            data = response.json()
            news_list = []
            
            # è§£ææ–°èé …ç›®
            items = data.get('items', {}).get('data', [])
            
            for item in items:
                try:
                    title = item.get('title', '')
                    news_id = item.get('newsId', '')
                    
                    # ç¯©é¸åŒ…å«è‚¡ç¥¨åç¨±æˆ–ä»£ç¢¼çš„æ–°è
                    if stock_name in title or stock_code in title:
                        link = f"https://news.cnyes.com/news/id/{news_id}"
                        summary = item.get('summary', '')[:100]
                        
                        news_list.append({
                            'title': title,
                            'snippet': summary,
                            'link': link,
                            'source': 'é‰…äº¨ç¶²'
                        })
                        
                        if len(news_list) >= 5:
                            break
                except Exception:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.error(f"é‰…äº¨ç¶²çˆ¬å–å¤±æ•—: {e}")
            return []
    
    def _fetch_yahoo_news(self, stock_code: str, stock_name: str) -> List[Dict]:
        """æŠ“å– Yahoo è‚¡å¸‚æ–°èï¼ˆå·²å»¢æ£„ï¼Œæ”¹ç”¨ RSSï¼‰"""
        return []
    
    def format_news_for_analysis(self, news_list: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ–°èä¾› AI åˆ†æ"""
        if not news_list:
            return "ç„¡ç›¸é—œæ–°è"
        
        formatted = "### è¿‘æœŸç›¸é—œæ–°è\n\n"
        for i, news in enumerate(news_list, 1):
            formatted += f"{i}. **{news['title']}**\n"
            formatted += f"   ä¾†æº: {news['source']}\n\n"
        
        return formatted
    
    def format_news_for_report(self, news_list: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ–°èä¾›å ±å‘Šé¡¯ç¤º"""
        if not news_list:
            return "ğŸ“° æš«ç„¡ç›¸é—œæ–°è\n"
        
        formatted = f"ğŸ“° è¿‘æœŸæ–°è ({len(news_list)} å‰‡)\n"
        formatted += "â”€" * 60 + "\n"
        
        for i, news in enumerate(news_list, 1):
            formatted += f"\n{i}. {news['title']}\n"
            formatted += f"   ä¾†æº: {news['source']}\n"
            formatted += f"   ğŸ”— {news['link']}\n"
        
        return formatted


def test_news_scraper():
    """æ¸¬è©¦æ–°èçˆ¬èŸ²"""
    print("=" * 60)
    print("ğŸ§ª æ¸¬è©¦å°ç£è²¡ç¶“æ–°èçˆ¬èŸ²")
    print("=" * 60)
    
    scraper = TaiwanNewsScaper()
    
    # æ¸¬è©¦å°ç©é›»
    print("\næœç´¢ã€Œå°ç©é›» (2330)ã€æ–°è...")
    news_list = scraper.search_stock_news(
        stock_code="2330",
        stock_name="å°ç©é›»",
        max_results=5
    )
    
    if news_list:
        print(f"âœ… æ‰¾åˆ° {len(news_list)} å‰‡æ–°è\n")
        print(scraper.format_news_for_report(news_list))
        return True
    else:
        print("âŒ æœªæ‰¾åˆ°æ–°è")
        return False


if __name__ == '__main__':
    test_news_scraper()
